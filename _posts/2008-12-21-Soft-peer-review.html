---
layout: post
title: Soft peer review
---
<div><br />        <p>The following paper &quot;Soft peer review: Social software and distributed<br /><br />scientific evaluation&quot; was passed along to me by alf today. I think<br /><br />another copy has been haunting my file system for a few days, but this<br /><br />seemed like a good reason to sit down again with it.</p><br /><br /><p>It's by <a href="http://nitens.org/taraborelli/home">Dario<br /><br />Taraborelli</a> and  the abstract is as follows:</p><br /><br /><p>Abstract: The debate on the prospects of peer-review in the Internet age and the<br /><br />increasing criticism leveled against the dominant role of impact factor<br /><br />indicators are calling for new measurable criteria to assess<br /><br />scientific quality.<br /><br />Usage-based metrics offer a new avenue to scientific quality assessment but<br /><br />face the same risks as first generation search engines that used unreliable<br /><br />metrics (such as raw traffic data) to estimate content quality. In<br /><br />this article I<br /><br />analyze the contribution that social bookmarking systems can provide to the<br /><br />problem of usage-based metrics for scientific evaluation. I suggest that<br /><br />collaboratively aggregated metadata may help fill the gap between traditional<br /><br />citation-based criteria and raw usage factors. I submit that bottom-up,<br /><br />distributed evaluation models such as those afforded by social bookmarking<br /><br />will challenge more traditional quality assessment models in terms of coverage,<br /><br />efficiency and scalability. Services aggregating user-related quality indicators<br /><br /> for online scientific content will come to occupy a key function in<br /><br />the scholarly<br /><br />communication system</p><br /><br /><p>and I get a mention in the acknowledgments, which is cool.</p><br /><br /><p>It is a very nice essay on the potential of social bookmarking as a tool for ran<br /><br />king academic articles, in addition to adding metadata to scientific articles. D<br /><br />ario discusses the issue of ranking the expertese of people who are bookmarking<br /><br />and proposes a really nice method to get over the scaling problem that is inherr<br /><br />ent when we try to intoduce manual methods to rank people. He suggestes that a u<br /><br />sers notes and annotations could be made available about a bookmark on an anonom<br /><br />ous basis. Others would have the option to copy these annotations, or rate them.<br /><br /> This would be a form of soft peer review on the annotations, which would in tur<br /><br />n effect the standing of the person creating these annotations.</p><br /><br /><p>There would be ways to cheat this system, but with enough signal, one hopes that<br /><br /> such noise could be drowned out.</p><br /><br /><p>The paper also pointed out <a href="http://www.naboj.com/">http://www.naboj.com/</a> which I'd not<br /><br />seen before and which is<br /><br />pretty amazing.</p><br /><br /><p>I really like this paper. Thanks Dario!</p>   <p> <br />    <a href="http://partiallyattended.vox.com/library/post/soft-peer-review.html?_c=feed-atom-full#comments">Read and post comments</a>   |   <br />    <a href="http://www.vox.com/share/6a00d09e7c9248be2b00fae8bdc438000b?_c=feed-atom-full">Send to a friend</a> <br /></p><br /><br />                </div>
